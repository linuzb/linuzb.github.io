<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Linuzb&#39; 的博客</title>
    <link>https://linuzb.github.io/posts/</link>
    <description>Recent content in Posts on Linuzb&#39; 的博客</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Tue, 09 Jul 2024 16:57:55 +0800</lastBuildDate>
    <atom:link href="https://linuzb.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jumpserver Frp Sealos</title>
      <link>https://linuzb.github.io/2024/07/jumpserver-frp-sealos/</link>
      <pubDate>Tue, 09 Jul 2024 16:57:55 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/07/jumpserver-frp-sealos/</guid>
      <description>前言 我们常常会遇到访问内网的场景（内网穿透）。JumpServer是广受欢迎的开源堡垒机，是符合 4A 规范的专业运维安全审计系统。我们可以使用 jumpserver 作为跳板机访问内网服务。&#xA;但是仅仅有 jumpserver 还不够，我们需要实现内网穿透，即通过公网访问内部的 jumpserver。因此这里用到了 fatedier/frp，一个反向代理工具，可帮助您将 NAT 或防火墙后的本地服务器接入互联网。&#xA;由于 jumpserver 部署在内网，且通过 http 暴露服务，如果我们直接使用 frp 代理 http 服务，很显然达不到安全的需求，因此我们还需要在公网使用一个网关，将 http 应用封装成 https 服务。一种实现方式是购买云主机和域名，部署 nginx 并通过 Let&amp;rsquo;s Encrypt Let&amp;rsquo;s Encrypt签发证书。但是这样涉及到金钱成本以及备案成本。我们暂不考虑。&#xA;这里介绍的暴露 frp 服务的方式是使用 Sealos Cloud。Sealos 是一个云上的 kubernetes 平台，我们可以直接在上面部署 frp pod，然后利用 Sealos 提供的公网网关来代理我们的服务。&#xA;部署 JumpServer 首先我们在内网的一台机器上部署 jumpserver。为了方便部署，我们使用 docker 部署&#xA;这里参考 jumpserver docker 部署文档。&#xA;1 2 3 4 5 6 7 8 # 测试环境可以使用，生产环境推荐外置数据 git clone --depth=1 https://github.com/jumpserver/Dockerfile.git cd Dockerfile cp config_example.</description>
    </item>
    <item>
      <title>Docker Compose Deploy Harbor</title>
      <link>https://linuzb.github.io/2024/06/docker-compose-deploy-harbor/</link>
      <pubDate>Wed, 05 Jun 2024 08:55:26 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/06/docker-compose-deploy-harbor/</guid>
      <description>下载 harbor 安装器 首先到官方 release 页面查找对应的版本号，使用以下命令下载&#xA;1 2 3 4 5 export HARBOR_VERSION=2.9.4 wget https://github.com/goharbor/harbor/releases/download/v${HARBOR_VERSION}/harbor-offline-installer-v${HARBOR_VERSION}.tgz tar xvf harbor-offline-installer-v${HARBOR_VERSION}.tgz cd harbor mv harbor.yml.tmp harbor.yml 编辑 harbor.yml 1 vim harbor.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 将 hostname 设置成自己的域名或者 ip hostname: 172.16.0.115 http: # port for http, default is 80. If https enabled, this port will redirect to https port # 修改端口 port: 6060 # 如果没有 https 可以将 https 相关的配置注释 # https related config # https: # # https port for harbor, default is 443 # port: 443 # # The path of cert and key files for nginx # certificate: /your/certificate/path # private_key: /your/private/key/path # The default data volume data_volume: /data 详细内容参考文档 Configure the Harbor YML File</description>
    </item>
    <item>
      <title>Github Action 简明教程</title>
      <link>https://linuzb.github.io/2024/05/github-action-%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</link>
      <pubDate>Mon, 06 May 2024 10:01:18 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/05/github-action-%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</guid>
      <description>前言 本文的目的是介绍一种自动化的构建、测试、部署流程。&#xA;我们在开发软件时，常常需要构建环境、编译、测试以及部署，这些工作都是简单重复的工作，一旦我们的软件及环境复杂，这些工作就会变成一个重复且枯燥的过程。例如，当我们使用了 Docker 之后，我们构建环境就变成了一个 docker build 的过程。每当我们对环境有所修改，我们都需要重新 docker build 一次。每当我们修改一次代码，都需要忍受长时间的编译和测试。 这些工作都有标准的命令，我们是否可以写一个脚本，每当我们对代码进行修改，就可以自动执行以上工作呢？ 当然可以，不过 Github Action 就是这么一个平台，通过 Github Action 我们可以自定义修改代码之后的工作流程（构建镜像、编译、测试和部署等）。&#xA;感兴趣的可以搜索下 DevOps 和 CI/CD(持续集成和持续部署)。这是一种软件开发理念，这里我们不做过多介绍，接下来的内容是 CI/CD 的一种实现。&#xA;这里看一下 Github Action官方定义&#xA;GitHub Actions 是一种持续集成和持续交付 (CI/CD) 平台，可用于自动执行生成、测试和部署管道。 您可以创建工作流程来构建和测试存储库的每个拉取请求，或将合并的拉取请求部署到生产环境。&#xA;Github Action 的组件 我们可以配置 Github Action workflow(工作流)，当仓库中发生特定的事件时可以触发该工作流。如下图所示，下图即为一个工作流，一个工作流包含多个作业，这些作业可以并行可以串行。每个作业中运行了我们定义的脚本（例如docker build、编译、测试等）。&#xA;GitHub Actions 主要有以下几个概念&#xA;Workflows：工作流，可以添加到存储库中的自动化过程。工作流由一个或多个作业组成，可以由事件调度或触发。 Event：事件，触发工作流的特定动作。例如，向存储库提交 pr 或 pull 请求。 Jobs：作业，在同一跑步器上执行的一组步骤。默认情况下，具有多个作业的工作流将并行运行这些作业。 Steps：步骤，可以在作业中运行命令的单个任务。步骤可以是操作，也可以是 shell 命令。作业中的每个步骤都在同一个运行程序上执行，从而允许该作业中的操作彼此共享数据。 Actions：操作是独立的命令，它们被组合成创建作业的步骤。操作是工作流中最小的可移植构建块。你可以创建自己的动作，或者使用 GitHub 社区创建的动作。 Runners：运行器，安装了 GitHub Actions 运行器应用程序的服务器。。Github 托管的运行器基于 Ubuntu Linux、Microsoft Windows 和 macOS，工作流中的每个作业都在一个新的虚拟环境中运行。 快速入门 本文是一个入门教程，仅起到抛砖引玉的作用，详细概念介绍和使用方法以可见参考文档或者自行搜索。</description>
    </item>
    <item>
      <title>Docker 入门文档</title>
      <link>https://linuzb.github.io/2024/04/docker-%E5%85%A5%E9%97%A8%E6%96%87%E6%A1%A3/</link>
      <pubDate>Mon, 29 Apr 2024 22:01:37 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/docker-%E5%85%A5%E9%97%A8%E6%96%87%E6%A1%A3/</guid>
      <description>前言 本文将简单介绍 Docker 的安装与使用过程&#xA;docker 安装 docker 官方文档提供了各个系统的安装方式，参考 Install Docker Engine，但是由于国内网络环境，使用官方方式会比较慢，因此国内环境不推荐这种方式安装。下面我们介绍另一种国内安装方式。&#xA;自动化的安装方式，本节内容源自 清华大学开源镜像站|Docker CE 软件仓库，其他安装方式可以参考该文档。&#xA;Docker 提供了一个自动配置与安装的脚本，支持 Debian、RHEL、SUSE 系列及衍生系统的安装。&#xA;以下内容假定&#xA;您为 root 用户，或有 sudo 权限，或知道 root 密码； 您系统上有 curl 或 wget 1 2 3 4 5 export DOWNLOAD_URL=&amp;#34;https://mirrors.tuna.tsinghua.edu.cn/docker-ce&amp;#34; # 如您使用 curl curl -fsSL https://get.docker.com/ | sh # 如您使用 wget wget -O- https://get.docker.com/ | sh 使用非 root 使用 docker 通过以上方式安装的docker只能通过 root 用户或者 sudo 权限使用。&#xA;接下来我们将配置非 root 用户使用 docker&#xA;创建 docker 用户组 1 sudo groupadd docker 将当前用户添加到 docker 用户组 1 sudo usermod -aG docker $USER 激活用户组配置 1 newgrp docker 测试 docker 命令 1 docker run hello-world 此命令将下载测镜像并在容器中运行。容器运行时，它会打印一条信息并退出。</description>
    </item>
    <item>
      <title>从零开始在Kubernetes上搭建Jupyterhub</title>
      <link>https://linuzb.github.io/2024/04/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%9C%A8kubernetes%E4%B8%8A%E6%90%AD%E5%BB%BAjupyterhub/</link>
      <pubDate>Mon, 29 Apr 2024 15:08:47 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%9C%A8kubernetes%E4%B8%8A%E6%90%AD%E5%BB%BAjupyterhub/</guid>
      <description>前言 Jupyterhub 允许用户通过网页与计算环境交互，用户可以使用浏览器实现在线或者离线的数据分析，从而摆脱本地计算环境的资源限制。有了 JupyterHub，你就可以管理多个用户的 jupyter 服务。&#xA;Kubernetes 是一个容器编排服务。简单来说，Kubernetes 可以管理一个集群（包含多个计算机），我们可以在 kubernetes 上部署应用，这些应用会被 Kubernetes 调度到集群中的某台机器上运行。用户只需要关心应用运行结果，而不必关心资源调度与分配的过程。&#xA;注: Kubernetes 又叫做 k8s, 原因是在 k 和 s 之间有 8 个字母。&#xA;那么我们是否可以将 Jupyterhub 和 Kubernetes 结合，用来支持多用户 jupyter 服务的申请，并且让Kubernetes 实现资源的管理与分配呢。&#xA;实际上，Jupyterhub 官方已经支持，这个项目叫做 Zero to JupyterHub with Kubernetes，感兴趣的可以直接看 jupyterhub 的官方文档。&#xA;本文将基于 Zero to JupyterHub with Kubernetes 官方文档较少的方法，定制一个适合多用户深度学习计算平台。&#xA;概念介绍 这里简单粗浅的介绍一些概念，以便于大家对后续流程的理解&#xA;默认大家已经理解操作系统、进程、存储的概念&#xA;虚拟机就是模拟出来的一台完整的计算机系统。一台电脑利用虚拟机可以模拟出很多机器。但是这里我们并不使用虚拟机，介绍虚拟机是为了引出另一个概念——容器化。&#xA;容器化和虚拟机类似，它也可以在一台机器上模拟出很多计算系统。但是容器化技术更轻量一些，后面我们介绍的多用户环境其实都是容器化技术模拟出来的。感兴趣的可以搜一下容器化和虚拟机的区别。这里我们只需要知道，我们将使用容器化技术为每个用户创建出独立且相互隔离的计算环境。&#xA;如何使用虚拟机想必大家都知道，例如 Vmware、virtualBox 等等。那么如何创建容器呢，那就不得不提大名鼎鼎的 Docker，感兴趣的可以了解一下。不过很遗憾的是，我们没有使用 docker 作为容器化部署的工具，而是使用更轻量更符合云原生标准的 containerd。感兴趣的可以了解一下这两个区别。&#xA;好了，前面介绍的只需要知道，我们将使用容器化技术来实现多用户独立的计算环境。接下来我们将介绍一些 k8s 的概念。&#xA;首先，Pod是 k8s 的最小执行单元。这句话怎么理解呢？如果将 k8s 看作是一个操作系统的话，那么 pod 就是这个操作系统的进程。大家知道进程是操作系统进行资源分配和调度的基本单位。同理，pod 也是 k8s 中资源分配和调度的基本单位。还是优点绕？说白了，pod就是我们前面提到的容器，只不过在 k8s 中叫 pod。一个 pod 就是一个独立的计算环境，可以理解成一台虚拟机。</description>
    </item>
    <item>
      <title>Kubernetes Nfs Provider</title>
      <link>https://linuzb.github.io/2024/04/kubernetes-nfs-provider/</link>
      <pubDate>Mon, 29 Apr 2024 10:47:07 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubernetes-nfs-provider/</guid>
      <description>前言 本文介绍如何使用 nfs 网络存储为 kubernetes 提供动态的PV创建。&#xA;本文使用的环境&#xA;操作系统 主机IP 数据存储目录 Ubuntu 172.16.0.100/24 /data/nfs 部署 step1. 在宿主机部署 NFS 服务 1 2 3 sudo apt update # 服务端 sudo apt install nfs-kernel-server step2. 创建共享目录 创建目录/data/nfs&#xA;1 mkdir -p /data/nfs step3. 宿主机配置 NFS export 1 echo -e &amp;#34;/data/nfs\t172.16.0.100/24(rw,sync,no_subtree_check,no_root_squash)&amp;#34; | sudo tee -a /etc/exports 配置完成后重启 nfs server&#xA;1 sudo systemctl restart nfs-kernel-server step4. 测试 nfs 服务可用 1 /sbin/showmount -e 172.16.0.100 部署 NFS Provider 使用helm 部署</description>
    </item>
    <item>
      <title>Jupyterhub Userguide</title>
      <link>https://linuzb.github.io/2024/04/jupyterhub-userguide/</link>
      <pubDate>Sun, 28 Apr 2024 16:33:37 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/jupyterhub-userguide/</guid>
      <description>前置准备 本文是 jupyterhub on kubernetes 的使用文档，本文描述的环境基于 jupyterhub on kubernetes&#xA;在上述文档中，我们部署的 kubernetes master 节点 nodeIP 为 172.16.0.100, 因此以下教程中使用的访问 ip 都是 172.16.0.100&#xA;环境准备 内网访问 可以访问内网的浏览器&#xA;登录 使用浏览器访问 http://172.16.0.100:30080/ 认证 点击 Singn in with Github，然后在 github 页面授权&#xA;说明：&#xA;这里仅使用 github oauth 做认证鉴权，暂无其它依赖，原理课参考文档 Authorizing OAuth apps - GitHub Docs github 账号的用户名需要为全字母及数字，暂不支持非字母数字之外的其他符号 应用选择 登录完成之后就会进入一个应用服务页面&#xA;选择一个合适的应用环境，然后点击底部的 start 按钮，即可进入环境 自定义应用 TODO： 待补充&#xA;应用环境介绍 功能介绍 界面简介 应用创建完成后，就会跳转到jupyterlab页面 介绍一下主要功能&#xA;启动一个luncher，即页面右侧部分，在 luncher 中可以打开各种工具 上传文件按钮，点击后弹出一个对话框，选择需要上传到文件即可。如果想上传一个目录，可以先在本地将目录打包成一个压缩包，然后上传。上传前可以先按照3. 切换到对应的工作目录。 工作目录，双击可打开文件以及文件夹。上传文件以及打开vscode都是以工作目录为基础，例如，当前工作目录为~/Projects/data, 则上传目标路径即为~/Projects/data 新建一个jupyter notebook 以工作目录为基础打开vscode 打开一个terminal，可执行linux命令 vscode 打开方式：在luncher 中点击vscode 图标即可打开</description>
    </item>
    <item>
      <title>Jupyterhub on Kubernetes</title>
      <link>https://linuzb.github.io/2024/04/jupyterhub-on-kubernetes/</link>
      <pubDate>Sun, 28 Apr 2024 15:57:55 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/jupyterhub-on-kubernetes/</guid>
      <description>前置准备 首先需要部署 kubernetes 集群，参考k8s deploy Helm | Installing Helm 安装 helm 存储：Longhorn provider 部署 Longhorn 参考 Longhorn deploy&#xA;接入 github oauth 本文使用 github oauth 作为认证方式，首先我们需要提前注册一个 GitHub OAuth 应用程序，请参阅 GitHub 有关注册应用程序的官方文档 registering an app.&#xA;然后我们得到三个信息&#xA;client-id client-secret oauthCallbackUrl 为了避免在配置中明文保存密码，我们将以上信息通过 secret 的形式存储在 Kubernetes 中，可以使用挂载 secret 的形式使用 secret 变量。&#xA;创建secret 接下来我们给 client-id 和 client-secret 创建secret&#xA;1 2 3 echo -n &amp;#34;client-id&amp;#34; | base64 echo -n &amp;#34;client-secret&amp;#34; | base64 echo -n &amp;#34;oauthCallbackUrl&amp;#34; | base64 将以上结果填充到用于创建 secret 的 yaml 中</description>
    </item>
    <item>
      <title>Kubernetes 部署 longhorn 存储</title>
      <link>https://linuzb.github.io/2024/04/kubernetes-%E9%83%A8%E7%BD%B2-longhorn-%E5%AD%98%E5%82%A8/</link>
      <pubDate>Sun, 28 Apr 2024 15:46:48 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubernetes-%E9%83%A8%E7%BD%B2-longhorn-%E5%AD%98%E5%82%A8/</guid>
      <description>背景 longhorn 是基于 Kubernetes 构建的云原生分布式存储&#xA;依赖 Installation Requirements&#xA;ubuntu 安装依赖 运行以下脚本，检查是否满足依赖&#xA;1 curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/scripts/environment_check.sh | bash 如果缺少依赖，需要安装依赖，例如&#xA;1 2 apt-get install open-iscsi apt-get install nfs-common kubectl 安装及 kubeconfig 配置&#xA;部署 helm Longhorn | Documentation&#xA;1 2 3 4 5 6 helm pull longhorn/longhorn --version 1.6.1 helm upgrade --install longhorn ./longhorn \ --namespace longhorn-system \ --create-namespace \ --version 1.6.1 \ --values config.yaml config.yaml&#xA;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 defaultSettings: # 配置默认数据存放地址 defaultDataPath: /data/longhorn service: ui: # -- Service type for Longhorn UI.</description>
    </item>
    <item>
      <title>kubernetes 监控部署</title>
      <link>https://linuzb.github.io/2024/04/kubernetes-%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Sun, 28 Apr 2024 15:09:33 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubernetes-%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2/</guid>
      <description>准备 首先需要部署 kubernetes 集群，参考k8s deploy helm 安装 prometheus 软件栈 Setting up Prometheus — NVIDIA GPU Telemetry 1.0.0 documentation&#xA;使用以下命令安装 prometheus&#xA;1 2 3 4 5 6 7 8 9 helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \ --namespace prometheus \ --create-namespace \ --set prometheus.service.type=NodePort \ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \ --set prometheusOperator.admissionWebhooks.patch.image.registry=registry.cn-hangzhou.aliyuncs.com \ --set prometheusOperator.admissionWebhooks.patch.image.repository=linuzb/kube-webhook-certgen \ --set kube-state-metrics.image.registry=registry.cn-hangzhou.aliyuncs.com \ --set kube-state-metrics.image.repository=linuzb/kube-state-metrics 安装 GPU 监控 DCGM 1 2 3 4 helm upgrade --install \ dcgm-exporter \ gpu-helm-charts/dcgm-exporter \ --values config.</description>
    </item>
    <item>
      <title>Kubespray 部署 kubernetes</title>
      <link>https://linuzb.github.io/2024/04/kubespray-%E9%83%A8%E7%BD%B2-kubernetes/</link>
      <pubDate>Sun, 28 Apr 2024 10:06:36 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubespray-%E9%83%A8%E7%BD%B2-kubernetes/</guid>
      <description>准备 本文介绍中共使用了四台物理机, 其中一台用户部署 k8s 的机器，另外三台作为 kubernetes 的节点&#xA;机器 ip 系统 用户名 备注 部署机 任意 Ubuntu linuzb 部署k8s机器，非k8s集群节点 kube-master-100 172.16.0.100 Ubutnu linuzb 初始化 k8s master 节点 kube-node-117 172.16.0.117 Ubutnu linuzb 初始化 k8s node kube-node-114 172.16.0.114 Ubutnu linuzb 后续加入的 k8s node kubespray 配置 自定义配置 1 2 3 4 git clone https://github.com/kubernetes-sigs/kubespray cd kubespray # 拷贝集群清单 cp -rfp inventory/sample inventory/mycluster inventory inventory/mycluster/inventory.ini&#xA;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # ## Configure &amp;#39;ip&amp;#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster.</description>
    </item>
  </channel>
</rss>
