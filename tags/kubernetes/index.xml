<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Linuzb&#39; 的博客</title>
    <link>https://linuzb.github.io/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Linuzb&#39; 的博客</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Mon, 29 Apr 2024 15:08:47 +0800</lastBuildDate>
    <atom:link href="https://linuzb.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>从零开始在Kubernetes上搭建Jupyterhub</title>
      <link>https://linuzb.github.io/2024/04/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%9C%A8kubernetes%E4%B8%8A%E6%90%AD%E5%BB%BAjupyterhub/</link>
      <pubDate>Mon, 29 Apr 2024 15:08:47 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%9C%A8kubernetes%E4%B8%8A%E6%90%AD%E5%BB%BAjupyterhub/</guid>
      <description>前言 Jupyterhub 允许用户通过网页与计算环境交互，用户可以使用浏览器实现在线或者离线的数据分析，从而摆脱本地计算环境的资源限制。有了 JupyterHub，你就可以管理多个用户的 jupyter 服务。&#xA;Kubernetes 是一个容器编排服务。简单来说，Kubernetes 可以管理一个集群（包含多个计算机），我们可以在 kubernetes 上部署应用，这些应用会被 Kubernetes 调度到集群中的某台机器上运行。用户只需要关心应用运行结果，而不必关心资源调度与分配的过程。&#xA;注: Kubernetes 又叫做 k8s, 原因是在 k 和 s 之间有 8 个字母。&#xA;那么我们是否可以将 Jupyterhub 和 Kubernetes 结合，用来支持多用户 jupyter 服务的申请，并且让Kubernetes 实现资源的管理与分配呢。&#xA;实际上，Jupyterhub 官方已经支持，这个项目叫做 Zero to JupyterHub with Kubernetes，感兴趣的可以直接看 jupyterhub 的官方文档。&#xA;本文将基于 Zero to JupyterHub with Kubernetes 官方文档较少的方法，定制一个适合多用户深度学习计算平台。&#xA;概念介绍 这里简单粗浅的介绍一些概念，以便于大家对后续流程的理解&#xA;默认大家已经理解操作系统、进程、存储的概念&#xA;虚拟机就是模拟出来的一台完整的计算机系统。一台电脑利用虚拟机可以模拟出很多机器。但是这里我们并不使用虚拟机，介绍虚拟机是为了引出另一个概念——容器化。&#xA;容器化和虚拟机类似，它也可以在一台机器上模拟出很多计算系统。但是容器化技术更轻量一些，后面我们介绍的多用户环境其实都是容器化技术模拟出来的。感兴趣的可以搜一下容器化和虚拟机的区别。这里我们只需要知道，我们将使用容器化技术为每个用户创建出独立且相互隔离的计算环境。&#xA;如何使用虚拟机想必大家都知道，例如 Vmware、virtualBox 等等。那么如何创建容器呢，那就不得不提大名鼎鼎的 Docker，感兴趣的可以了解一下。不过很遗憾的是，我们没有使用 docker 作为容器化部署的工具，而是使用更轻量更符合云原生标准的 containerd。感兴趣的可以了解一下这两个区别。&#xA;好了，前面介绍的只需要知道，我们将使用容器化技术来实现多用户独立的计算环境。接下来我们将介绍一些 k8s 的概念。&#xA;首先，Pod是 k8s 的最小执行单元。这句话怎么理解呢？如果将 k8s 看作是一个操作系统的话，那么 pod 就是这个操作系统的进程。大家知道进程是操作系统进行资源分配和调度的基本单位。同理，pod 也是 k8s 中资源分配和调度的基本单位。还是优点绕？说白了，pod就是我们前面提到的容器，只不过在 k8s 中叫 pod。一个 pod 就是一个独立的计算环境，可以理解成一台虚拟机。</description>
    </item>
    <item>
      <title>Jupyterhub Userguide</title>
      <link>https://linuzb.github.io/2024/04/jupyterhub-userguide/</link>
      <pubDate>Sun, 28 Apr 2024 16:33:37 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/jupyterhub-userguide/</guid>
      <description>前置准备 本文是 jupyterhub on kubernetes 的使用文档，本文描述的环境基于 jupyterhub on kubernetes&#xA;在上述文档中，我们部署的 kubernetes master 节点 nodeIP 为 172.16.0.100, 因此以下教程中使用的访问 ip 都是 172.16.0.100&#xA;环境准备 内网访问 可以访问内网的浏览器&#xA;登录 使用浏览器访问 http://172.16.0.100:30080/ 认证 点击 Singn in with Github，然后在 github 页面授权&#xA;说明：&#xA;这里仅使用 github oauth 做认证鉴权，暂无其它依赖，原理课参考文档 Authorizing OAuth apps - GitHub Docs github 账号的用户名需要为全字母及数字，暂不支持非字母数字之外的其他符号 应用选择 登录完成之后就会进入一个应用服务页面&#xA;选择一个合适的应用环境，然后点击底部的 start 按钮，即可进入环境 自定义应用 TODO： 待补充&#xA;应用环境介绍 功能介绍 界面简介 应用创建完成后，就会跳转到jupyterlab页面 介绍一下主要功能&#xA;启动一个luncher，即页面右侧部分，在 luncher 中可以打开各种工具 上传文件按钮，点击后弹出一个对话框，选择需要上传到文件即可。如果想上传一个目录，可以先在本地将目录打包成一个压缩包，然后上传。上传前可以先按照3. 切换到对应的工作目录。 工作目录，双击可打开文件以及文件夹。上传文件以及打开vscode都是以工作目录为基础，例如，当前工作目录为~/Projects/data, 则上传目标路径即为~/Projects/data 新建一个jupyter notebook 以工作目录为基础打开vscode 打开一个terminal，可执行linux命令 vscode 打开方式：在luncher 中点击vscode 图标即可打开</description>
    </item>
    <item>
      <title>Jupyterhub on Kubernetes</title>
      <link>https://linuzb.github.io/2024/04/jupyterhub-on-kubernetes/</link>
      <pubDate>Sun, 28 Apr 2024 15:57:55 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/jupyterhub-on-kubernetes/</guid>
      <description>前置准备 首先需要部署 kubernetes 集群，参考k8s deploy Helm | Installing Helm 安装 helm 存储：Longhorn provider 部署 Longhorn 参考 Longhorn deploy&#xA;接入 github oauth 本文使用 github oauth 作为认证方式，首先我们需要提前注册一个 GitHub OAuth 应用程序，请参阅 GitHub 有关注册应用程序的官方文档 registering an app.&#xA;然后我们得到三个信息&#xA;client-id client-secret oauthCallbackUrl 为了避免在配置中明文保存密码，我们将以上信息通过 secret 的形式存储在 Kubernetes 中，可以使用挂载 secret 的形式使用 secret 变量。&#xA;创建secret 接下来我们给 client-id 和 client-secret 创建secret&#xA;1 2 3 echo -n &amp;#34;client-id&amp;#34; | base64 echo -n &amp;#34;client-secret&amp;#34; | base64 echo -n &amp;#34;oauthCallbackUrl&amp;#34; | base64 将以上结果填充到用于创建 secret 的 yaml 中</description>
    </item>
    <item>
      <title>Kubernetes 部署 longhorn 存储</title>
      <link>https://linuzb.github.io/2024/04/kubernetes-%E9%83%A8%E7%BD%B2-longhorn-%E5%AD%98%E5%82%A8/</link>
      <pubDate>Sun, 28 Apr 2024 15:46:48 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubernetes-%E9%83%A8%E7%BD%B2-longhorn-%E5%AD%98%E5%82%A8/</guid>
      <description>背景 longhorn 是基于 Kubernetes 构建的云原生分布式存储&#xA;依赖 Installation Requirements&#xA;ubuntu 安装依赖 运行以下脚本，检查是否满足依赖&#xA;1 curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/scripts/environment_check.sh | bash 如果缺少依赖，需要安装依赖，例如&#xA;1 2 apt-get install open-iscsi apt-get install nfs-common kubectl 安装及 kubeconfig 配置&#xA;部署 helm Longhorn | Documentation&#xA;1 2 3 4 5 6 helm pull longhorn/longhorn --version 1.6.1 helm upgrade --install longhorn ./longhorn \ --namespace longhorn-system \ --create-namespace \ --version 1.6.1 \ --values config.yaml config.yaml&#xA;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 defaultSettings: # 配置默认数据存放地址 defaultDataPath: /data/longhorn service: ui: # -- Service type for Longhorn UI.</description>
    </item>
    <item>
      <title>kubernetes 监控部署</title>
      <link>https://linuzb.github.io/2024/04/kubernetes-%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Sun, 28 Apr 2024 15:09:33 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubernetes-%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2/</guid>
      <description>准备 首先需要部署 kubernetes 集群，参考k8s deploy helm 安装 prometheus 软件栈 Setting up Prometheus — NVIDIA GPU Telemetry 1.0.0 documentation&#xA;使用以下命令安装 prometheus&#xA;1 2 3 4 5 6 7 8 9 helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \ --namespace prometheus \ --create-namespace \ --set prometheus.service.type=NodePort \ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \ --set prometheusOperator.admissionWebhooks.patch.image.registry=registry.cn-hangzhou.aliyuncs.com \ --set prometheusOperator.admissionWebhooks.patch.image.repository=linuzb/kube-webhook-certgen \ --set kube-state-metrics.image.registry=registry.cn-hangzhou.aliyuncs.com \ --set kube-state-metrics.image.repository=linuzb/kube-state-metrics 安装 GPU 监控 DCGM 1 2 3 4 helm upgrade --install \ dcgm-exporter \ gpu-helm-charts/dcgm-exporter \ --values config.</description>
    </item>
    <item>
      <title>Kubespray 部署 kubernetes</title>
      <link>https://linuzb.github.io/2024/04/kubespray-%E9%83%A8%E7%BD%B2-kubernetes/</link>
      <pubDate>Sun, 28 Apr 2024 10:06:36 +0800</pubDate>
      <guid>https://linuzb.github.io/2024/04/kubespray-%E9%83%A8%E7%BD%B2-kubernetes/</guid>
      <description>准备 本文介绍中共使用了四台物理机, 其中一台用户部署 k8s 的机器，另外三台作为 kubernetes 的节点&#xA;机器 ip 系统 用户名 备注 部署机 任意 Ubuntu linuzb 部署k8s机器，非k8s集群节点 kube-master-100 172.16.0.100 Ubutnu linuzb 初始化 k8s master 节点 kube-node-117 172.16.0.117 Ubutnu linuzb 初始化 k8s node kube-node-114 172.16.0.114 Ubutnu linuzb 后续加入的 k8s node kubespray 配置 自定义配置 1 2 3 4 git clone https://github.com/kubernetes-sigs/kubespray cd kubespray # 拷贝集群清单 cp -rfp inventory/sample inventory/mycluster inventory inventory.ini&#xA;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # ## Configure &amp;#39;ip&amp;#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster.</description>
    </item>
  </channel>
</rss>
